{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Transformer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLqTLSe0OFh"
      },
      "source": [
        "# Text classification with Transformer\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/10<br>\n",
        "**Last modified:** 2020/05/10<br>\n",
        "**Description:** Implement a Transformer block as a Keras layer and use it for text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8n2qOId0OFj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ9hG5g60OFl"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heaV5DLy0OFm"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw3sp11G0OFn"
      },
      "source": [
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6H0ZeeU0OFo"
      },
      "source": [
        "## Implement embedding layer\n",
        "\n",
        "Two seperate embedding layers, one for tokens, one for token index (positions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Xks28Z0OFp"
      },
      "source": [
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgwAJKQg0OFr"
      },
      "source": [
        "## Download and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGNgg46ladxD",
        "outputId": "8c5ec5e0-ec5c-41b7-abdd-e037505fe8ae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiBXnBCkaj2f"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/T2T/Data /content/T2T"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCX5NlSJamiy",
        "outputId": "1db3b8f8-542e-45be-b2da-47f3edf36939"
      },
      "source": [
        "!pip install tensor2tensor"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 14.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 10.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 8.6MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 8.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 307kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 563kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 573kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 593kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 604kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 614kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 839kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 849kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 870kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 880kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 890kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 901kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 911kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 921kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.41.1)\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.16.0)\n",
            "Collecting mesh-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/10/37df0bc87ebf84e1414613176340e3aadc3697d2bd112bf63d3d4b1e848a/mesh_tensorflow-0.1.19-py3-none-any.whl (366kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (2.23.0)\n",
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.0.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.4.0)\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.12.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.1.2.30)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.0.5)\n",
            "Collecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.1.4)\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 36.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 31.3MB/s \n",
            "\u001b[?25hCollecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 20.7MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/dd/5b190393e6066286773a67dfcc2f9492058e9b57c4867a95f1ba5caf0a83/gunicorn-20.1.0-py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.7.1)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (0.17.3)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (4.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (3.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor) (1.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor) (1.24.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor) (0.12.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (21.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (0.3.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (0.1.6)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (5.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (3.17.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensor2tensor) (1.1.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/a7/94e1a92c71436f934cdd2102826fa041c83dcb7d21dd0f1fb1a57f6e0620/zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor) (57.2.0)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (1.32.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor) (1.26.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor) (2.11.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor) (2.7.1)\n",
            "Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor) (4.4.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor) (1.2.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->tensor2tensor) (1.5.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tensor2tensor) (0.4.8)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->tensor2tensor) (1.5.2)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tensor2tensor) (3.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->tensor2tensor) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor) (4.2.2)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (21.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->tensor2tensor) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor) (2.4.7)\n",
            "Building wheels for collected packages: bz2file, pypng\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp37-none-any.whl size=6884 sha256=da058f7fc9d22d29164cc4050975f91aab7b2eeebb2c31271e6abb5b60e8e951\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp37-none-any.whl size=67179 sha256=92b850becc385dda5aaaa56be2591a4dba0f7ff0501d8139c120008f370c7f2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "Successfully built bz2file pypng\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-probability, kfac, mesh-tensorflow, tensorflow-gan, zope.interface, zope.event, gevent, bz2file, tensorflow-addons, pypng, gunicorn, tf-slim, tensor2tensor\n",
            "  Found existing installation: tensorflow-probability 0.13.0\n",
            "    Uninstalling tensorflow-probability-0.13.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.13.0\n",
            "Successfully installed bz2file-0.98 gevent-21.1.2 gunicorn-20.1.0 kfac-0.2.3 mesh-tensorflow-0.1.19 pypng-0.0.20 tensor2tensor-1.15.7 tensorflow-addons-0.13.0 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_CiT8PE0OFs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "0b3bbbf2-0169-49d1-8b3f-1643a63f3a1a"
      },
      "source": [
        "vocab_size = 1800  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first x words of each sample  -- What should I set this to?\n",
        "\n",
        "\n",
        "training_set = [\"/content/T2T/EFC401_B4.tfrecord\", \"/content/T2T/EFC401_B6.tfrecord\", \"/content/T2T/EFC401_B8.tfrecord\", \"/content/T2T/EFC401_B12.tfrecord\", \"/content/T2T/EFC401_B17.tfrecord\", \"/content/T2T/EFC401_B18.tfrecord\", \"/content/T2T/EFC401_B20.tfrecord\", \"/content/T2T/EFC401_B32.tfrecord\", \"/content/T2T/EFC401_B34.tfrecord\", \"/content/T2T/EFC401_B41.tfrecord\", \"/content/T2T/EFC401_B57.tfrecord\", \"/content/T2T/EFC401_B61.tfrecord\", \"/content/T2T/EFC401_B66.tfrecord\", \"/content/T2T/EFC401_B69.tfrecord\",\"/content/T2T/EFC401_B73.tfrecord\", \"/content/T2T/EFC401_B77.tfrecord\"]\n",
        "testing_set = [\"/content/T2T/EFC401_B87.tfrecord\"]   \n",
        "validation_set = [\"/content/T2T/EFC401_B83.tfrecord\"]\n",
        "train_dataset, test_dataset, validation_dataset = tf.data.TFRecordDataset(training_set), tf.data.TFRecordDataset(testing_set), tf.data.TFRecordDataset(validation_set)\n",
        "print(train_dataset)\n",
        "\n",
        "#train_dataset, test_dataset, validation_dataset = tf.data.TFRecordDataset(training_set), tf.data.TFRecordDataset(testing_set), tf.data.TFRecordDataset(validation_set)\n",
        "\n",
        "#(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "#print(len(train_dataset), \"Training sequences\") -- error, dataset length is unknown/infinite.  Can I just ignore this...\n",
        "#print(len(validation_dataset), \"Validation sequences\")\n",
        "train_dataset = keras.preprocessing.sequence.pad_sequences(training_set)\n",
        "validation_dataset = keras.preprocessing.sequence.pad_sequences(validation_dataset)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TFRecordDatasetV2 shapes: (), types: tf.string>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e9406a5e5599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#print(len(train_dataset), \"Training sequences\") -- error, dataset length is unknown/infinite.  Can I just ignore this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(len(validation_dataset), \"Validation sequences\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    152\u001b[0m   return sequence.pad_sequences(\n\u001b[1;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m keras_export(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/content/T2T/EFC401_B4.tfrecord'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKup9bWZ0OFu"
      },
      "source": [
        "## Create classifier model using transformer layer\n",
        "\n",
        "Transformer layer outputs one vector for each time step of our input sequence.\n",
        "Here, we take the mean across all time steps and\n",
        "use a feed forward network on top of it to classify text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN0842Mz0OFu"
      },
      "source": [
        "\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCbwrhAi0OFw"
      },
      "source": [
        "## Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdprOQCw0OFx"
      },
      "source": [
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}